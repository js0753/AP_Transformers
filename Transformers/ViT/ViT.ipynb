{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AQ24W9Lf_p-s"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shahj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 970
        },
        "id": "7xKe_1qeacf-",
        "outputId": "eabb0aca-55b7-4cb6-9a6c-902d105012dc"
      },
      "outputs": [],
      "source": [
        "# Define the number of patches and the patch size\n",
        "num_patches = 196  # 14 x 14 patches for a 224 x 224 image\n",
        "patch_size = 16    # 16 x 16 pixels for each patch\n",
        "num_channels = 3   # 3 channels for RGB images\n",
        "\n",
        "# Define the train and val transforms\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.view(-1, patch_size * patch_size * num_channels)),\n",
        "])\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.view(-1, patch_size * patch_size * num_channels)),\n",
        "])\n",
        "\n",
        "# Load the Tiny ImageNet dataset\n",
        "train_dataset = ImageFolder('timn/train', transform=train_transforms)\n",
        "val_dataset = ImageFolder('timn/val', transform=val_transforms)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8PwtT1k1WD04"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create data loaders for training and validation data\n",
        "# Define the train and val loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for i,j in train_loader:\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4bo6pUoZ2Rwr"
      },
      "outputs": [],
      "source": [
        "class ClassToken():\n",
        "  def __init__(self,batch_size,dim):\n",
        "    self.token = np.random.normal(scale=0.1, size=(batch_size,1, dim))\n",
        "  \n",
        "  def prepend(self,words):\n",
        "    print(\"X shape is : \",words.shape)\n",
        "    print(\"Token shape is : \",self.token.shape)\n",
        "    return np.concatenate((self.token,words),axis=1) # [batch_size,196,768]+[batch_size,1,768]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oEHa-l0qAl_V"
      },
      "outputs": [],
      "source": [
        "def softmax(X,axis):\n",
        "  exp_X = np.exp(X)\n",
        "  return exp_X/np.sum(exp_X,axis=axis,keepdims=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "UjRlTahXgjHU"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  def attention(self,q,k,v):\n",
        "    dk = len(q[0])\n",
        "    attn= np.matmul(softmax(np.matmul(q,np.swapaxes(k, -1, -2))/np.sqrt(dk),axis=-1),v) # (1,dk) x (dk,1) x (1,dv) = (1,1) x (1,dv) = (1,dv)\n",
        "    print(\"Attention shape : \",attn.shape)\n",
        "    return attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "iKRpxvERczvm"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention():\n",
        "  def __init__(self,dmodel,h):\n",
        "    self.h=h\n",
        "    self.dk = dmodel//h \n",
        "    self.dv = dmodel//h\n",
        "    self.WQ = []\n",
        "    self.WK = []\n",
        "    self.WV = []\n",
        "    self.WO = np.random.normal(scale=0.1, size=(h*self.dv, dmodel))\n",
        "    self.sdpa = ScaledDotProductAttention()\n",
        "    self.WQ = np.random.normal(scale=0.1, size=(h,dmodel, self.dk))\n",
        "    self.WK = np.random.normal(scale=0.1, size=(h,dmodel, self.dk))\n",
        "    self.WV = np.random.normal(scale=0.1, size=(h,dmodel, self.dv))\n",
        "    self.WQ = np.array(self.WQ)\n",
        "    self.WK = np.array(self.WK)\n",
        "    self.WV = np.array(self.WV)\n",
        "    print(\"WQ shape : \",self.WQ.shape)\n",
        "    pass\n",
        "\n",
        "  def MultiHead(self,Q,K,V):\n",
        "    head = []\n",
        "    q = np.matmul(Q,self.WQ) # (1,197,768) * (8,768,768/8) -> (8,197,768/8)\n",
        "    print(\"Query shape : \",q.shape)\n",
        "    k = np.matmul(K,self.WK)\n",
        "    v = np.matmul(V,self.WV)\n",
        "    head= self.sdpa.attention(q,k,v)\n",
        "    head = np.concatenate(head,axis=1)\n",
        "    print(\"Head shape : \",head.shape)\n",
        "    multihead = np.dot(head,self.WO) # (dmodel,dv*h)\n",
        "    print(\"Multi-Head Shape : \",multihead.shape)\n",
        "    return multihead\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ua_bX8Pvc6HW"
      },
      "outputs": [],
      "source": [
        "class AddAndNorm():\n",
        "  def __init__(self,feat_size):\n",
        "    self.epsilon = 1e-8\n",
        "    self.gamma = np.ones(feat_size)\n",
        "    self.beta = np.zeros(feat_size)\n",
        "  def add(self,x,y):\n",
        "    return x+y\n",
        "  def norm(self,x):\n",
        "    #print(\"Norming\")\n",
        "    return np.multiply(self.gamma,(x-np.mean(x))) / np.sqrt(np.var(x)+self.epsilon)+self.beta\n",
        "\n",
        "class Norm():\n",
        "  def __init__(self,feat_size):\n",
        "    self.epsilon = 1e-8\n",
        "    self.gamma = np.ones(feat_size)\n",
        "    self.beta = np.zeros(feat_size)\n",
        "  def norm(self,x):\n",
        "    #print(\"Norming\")\n",
        "    return np.multiply(self.gamma,(x-np.mean(x))) / np.sqrt(np.var(x)+self.epsilon)+self.beta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "HLbWuRUxc_g2"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math\n",
        "class FeedForward():\n",
        "  def __init__(self,in_dim,out_dim):\n",
        "    self.out_dim = out_dim\n",
        "    self.W = np.random.normal(scale=np.sqrt(2.0 / in_dim), size=(in_dim, out_dim))\n",
        "    self.b = np.zeros((out_dim,)) \n",
        "\n",
        "  def forward(self,X):\n",
        "    return np.dot(X,self.W)+self.b\n",
        "  \n",
        "class MLP():\n",
        "  def __init__(self,d_model,d_ff,activation):\n",
        "    self.d_ff=d_ff\n",
        "    self.ff1 = FeedForward(d_model,d_ff)\n",
        "    self.ff2 = FeedForward(d_ff,d_model)\n",
        "    if activation == \"GELU\":\n",
        "      self.activation = self.GELU\n",
        "    elif activation == \"RELU\":\n",
        "      self.activation = self.RELU\n",
        "\n",
        "  def RELU(self,x):\n",
        "    return np.maximum(x,0)\n",
        "  \n",
        "  def GELU(self,x):\n",
        "    # return 0.5 * x * (1+self.erf(x/np.sqrt(2)))\n",
        "    return 0.5 * x * (1+np.vectorize(math.erf)(x/np.sqrt(2)))\n",
        "  \n",
        "  def forward(self,x):\n",
        "    return self.GELU(self.ff2.forward(self.GELU(self.ff1.forward(x))))\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def CrossEntropyLoss(y_pred,y_true):\n",
        "    n = len(y_pred)\n",
        "    return (-1/n) * np.dot(y_true,np.log(y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "wGev7DpidFkV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "196\n",
            "WQ shape :  (12, 768, 64)\n"
          ]
        }
      ],
      "source": [
        "class Encoder():\n",
        "  def __init__(self,batch_size=1,vocab_size=196,dmodel=768,h=12,d_ff=3072,num_classes = 200):\n",
        "    # self.ie = InputEmbedding(vocab_size,dmodel)\n",
        "    self.mha = MultiHeadAttention(dmodel,h)\n",
        "    self.ffn = MLP(d_ff=d_ff,d_model=dmodel,activation='GELU')\n",
        "    self.an = AddAndNorm((1,dmodel))\n",
        "    self.cls = ClassToken(batch_size=batch_size,dim=dmodel)\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "  def encode(self,X):\n",
        "    X = self.cls.prepend(X)\n",
        "    print(\"Prepended Class Token, Shape: \",X.shape)\n",
        "    X = self.an.norm(X)\n",
        "    multi_head_op = self.mha.MultiHead(X,X,X)\n",
        "    print(\"Performed Multi-Headed Attention\")\n",
        "    print(\"Multi Head OP shape : \",multi_head_op.shape)\n",
        "    mhan=self.an.norm(self.an.add(X,multi_head_op))\n",
        "    ffn_op = self.ffn.forward(mhan)\n",
        "    print(\"Passed through Feed-Forward Layer\")\n",
        "    op =self.an.add(mhan,ffn_op)\n",
        "    return op \n",
        "  \n",
        "  def loss(self,y_true):\n",
        "    one_hot_encoded = np.zeros((self.batch_size,self.num_classes))\n",
        "    one_hot_encoded[np.arange(self.batch_size),y_true]=1 #set the value at index [i, y_true[i]]\n",
        "    return CrossEntropyLoss(self.class_prob,one_hot_encoded)\n",
        "\n",
        "\n",
        "  def pred(self,input):\n",
        "    encoded = self.encode(input)\n",
        "    X = encoded[:,0,:]\n",
        "    classification_head = FeedForward(in_dim = X.shape[-1],out_dim =self.num_classes ) # 200 output classes\n",
        "    self.logits = classification_head.forward(X)\n",
        "    self.class_prob = softmax(self.logits, axis=-1)\n",
        "    # print(\"Logits Shape : \",self.logits.shape) #Just because dimension of op matches doesn't mean it's right, axis = 0 would also give op (1,200) but it's not right, correct is axis = 1 i.e., last axis=-1.\n",
        "    # print(\"Len Logits.shape - 1 is \",len(logits.shape)-1) #Not len(logits)-1\n",
        "    # print(\"Class Prob Shape : \",class_prob.shape)\n",
        "    y = np.argmax(self.class_prob,axis=-1)\n",
        "    return y\n",
        "\n",
        "vocab_size = 224*224 // (16*16)\n",
        "print(vocab_size)\n",
        "encoder = Encoder(vocab_size=vocab_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 196, 768])\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "\n",
        "# Get the first image from the train loader\n",
        "images, labels = next(iter(train_loader))\n",
        "\n",
        "# Print the shape of the image tensor\n",
        "print(images.shape)\n",
        "input = images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 196, 768)\n"
          ]
        }
      ],
      "source": [
        "print(images.numpy().shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oibncyAiBawE",
        "outputId": "a3248a7b-787b-443f-f784-0ea2e0ab6dee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X shape is :  (1, 196, 768)\n",
            "Token shape is :  (1, 1, 768)\n",
            "Prepended Class Token, Shape:  (1, 197, 768)\n",
            "Query shape :  (12, 197, 64)\n",
            "Attention shape :  (12, 197, 64)\n",
            "Head shape :  (197, 768)\n",
            "Multi-Head Shape :  (197, 768)\n",
            "Performed Multi-Headed Attention\n",
            "Multi Head OP shape :  (197, 768)\n",
            "Passed through Feed-Forward Layer\n",
            "[[[-1.50389248  0.98091977  0.18712773 ... -2.05894384 -1.38151589\n",
            "    1.18308271]\n",
            "  [-1.74451334  0.78367286  0.79379531 ... -1.38347247 -1.44708771\n",
            "    0.79965622]\n",
            "  [-1.86668981  0.77823345  0.67356099 ... -1.07753287 -1.1477566\n",
            "    0.45648206]\n",
            "  ...\n",
            "  [-0.24601088  1.41364879 -2.3731157  ...  1.40852449  0.30223751\n",
            "   -1.05254641]\n",
            "  [ 0.05653893 -0.2478573  -1.16180478 ...  0.279428   -0.20224152\n",
            "   -0.12608957]\n",
            "  [-1.26971275  0.12931689  0.19381513 ... -0.01116053 -1.06232817\n",
            "    0.88437635]]]\n"
          ]
        }
      ],
      "source": [
        "encoder_op = encoder.encode(images.numpy())\n",
        "print(encoder_op)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTmN_1FoBgcs",
        "outputId": "eed3c8b2-1c97-4639-e905-3e589b888dd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 197, 768)\n"
          ]
        }
      ],
      "source": [
        "print(encoder_op.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X shape is :  (1, 196, 768)\n",
            "Token shape is :  (1, 1, 768)\n",
            "Prepended Class Token, Shape:  (1, 197, 768)\n",
            "Query shape :  (12, 197, 64)\n",
            "Attention shape :  (12, 197, 64)\n",
            "Head shape :  (197, 768)\n",
            "Multi-Head Shape :  (197, 768)\n",
            "Performed Multi-Headed Attention\n",
            "Multi Head OP shape :  (197, 768)\n",
            "Passed through Feed-Forward Layer\n"
          ]
        }
      ],
      "source": [
        "pred = encoder.pred(images.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1,)\n"
          ]
        }
      ],
      "source": [
        "print(pred.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[35]\n"
          ]
        }
      ],
      "source": [
        "print(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def normalizer(dataset):\n",
        "#     mean = np.zeros(3)\n",
        "#     std = np.zeros(3)\n",
        "#     for images,_ in dataset:\n",
        "#         print(np.mean(images.numpy(),axis=(0,1))\n",
        "#         mean+= np.mean(images.numpy(),axis=(0,1))\n",
        "#         std += np.std(images.numpy(),axis=(0,1))\n",
        "#     mean /= len(dataset)\n",
        "#     std /= len(dataset)\n",
        "\n",
        "#     normalized_dataset = transforms.Normalize(mean=mean,std=std)(dataset)\n",
        "#     return normalized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_dataset = normalizer(train_dataset)\n",
        "# val_dataset = normalizer(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from scipy.integrate import quadrature\n",
        "# def f(self,t):\n",
        "  #   return np.exp(-t**2)\n",
        "\n",
        "  # def MonteCarlo(self,x,a,b):\n",
        "  #   N=10000\n",
        "  #   sample = np.random.uniform(a,b,N)\n",
        "  #   y = self.f(sample)\n",
        "  #   I = (b-a)*np.mean(y)\n",
        "  #   return I\n",
        "\n",
        "  # def gaussian_quadrature(self,x):\n",
        "  #   return quadrature(self.f,0,x)\n",
        "  \n",
        "  # def erf(self,x):\n",
        "  #   I,_ = np.vectorize(self.gaussian_quadrature)(x)\n",
        "  #   # I = self.MonteCarlo(0,x)\n",
        "  #   return (2*np.pi)* (I)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
