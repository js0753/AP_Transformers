{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN7oI3ObKdwF",
        "outputId": "5a88466a-0dfb-494e-de40-3216369a11d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (4.3.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.4\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnPknJkbPnTW",
        "outputId": "4c5a9a37-8513-4c39-e9b5-a89f23a49aab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 KB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.10.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.11.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.8.2\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVqIRlzLOzX-"
      },
      "outputs": [],
      "source": [
        "# from gensim.models import KeyedVectors\n",
        "# model = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/APTF/Data/GoogleNews-vectors-negative300.bin', binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csRpA0BoPpkW"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "# dataset = load_dataset('wmt14','de-en',split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_Bk9HyD9t0O"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# from nltk.corpus import stopwords\n",
        "\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# nltk.download('punkt')\n",
        "# def encode_sentence(sentence, model):\n",
        "#     # Tokenize the sentence\n",
        "#     tokens = word_tokenize(sentence.lower())\n",
        "\n",
        "#     # Remove stop words\n",
        "#     stop_words = set(stopwords.words('english'))\n",
        "#     tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "#     # Encode each word with word2vec\n",
        "#     embeddings = []\n",
        "#     for token in tokens:\n",
        "#         if token in model.key_to_index:\n",
        "#             embeddings.append(model.get_vector(token))\n",
        "\n",
        "#     # Combine the word embeddings\n",
        "#     if embeddings:\n",
        "#         return np.mean(embeddings, axis=0)\n",
        "#     else:\n",
        "#         return None\n",
        "\n",
        "# # Example usage\n",
        "# sentence = \"This is an apple.\"\n",
        "# embedding = encode_sentence(sentence, model)\n",
        "# print(embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "ajSDNymwSwiq",
        "outputId": "d9a73112-e1ac-4916-bccb-98b6df293f3e"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-ca85f60b6052>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'embedding' is not defined"
          ]
        }
      ],
      "source": [
        "# print(embedding.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ24W9Lf_p-s"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from tokenizers import ByteLevelBPETokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnaxAjyA_jOM"
      },
      "outputs": [],
      "source": [
        "save_dir = \"/content/drive/MyDrive/APTF/Data/tokenizer\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRz3AhV-U36L",
        "outputId": "7c7b6533-8a74-4f67-d756-fe3df192f9e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/APTF/Data/tokenizer/vocab.json',\n",
              " '/content/drive/MyDrive/APTF/Data/tokenizer/merges.txt']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# import os\n",
        "# from pathlib import Path\n",
        "# from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "# # Load the TSV file\n",
        "# path = Path(\"/content/drive/MyDrive/APTF/Data/tatoeba-en-de.tsv\")\n",
        "# with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "#     data = f.readlines()\n",
        "\n",
        "# # Train a ByteLevelBPETokenizer on the TSV data\n",
        "# tokenizer = ByteLevelBPETokenizer()\n",
        "# tokenizer.train_from_iterator(data)\n",
        "# # Save the tokenizer model to a file\n",
        "# os.makedirs(\"/content/drive/MyDrive/APTF/Data/tokenizer\", exist_ok=True)\n",
        "# save_dir = \"/content/drive/MyDrive/APTF/Data/tokenizer\"\n",
        "# tokenizer.save_model(save_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rp1XdsjU8m6",
        "outputId": "834a55c2-c559-4cad-a59a-238474229df9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['This', 'Ġis', 'Ġan', 'Ġapple']\n"
          ]
        }
      ],
      "source": [
        "# Load the trained tokenizer model\n",
        "tokenizer = ByteLevelBPETokenizer.from_file(save_dir+\"/vocab.json\", save_dir+\"/merges.txt\")\n",
        "\n",
        "# Encode a new sentence with BPE\n",
        "encoded = tokenizer.encode(\"This is an apple\")\n",
        "print(encoded.tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xKe_1qeacf-"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bo6pUoZ2Rwr"
      },
      "outputs": [],
      "source": [
        "class InputEmbedding():\n",
        "  def __init__(self,v,d):\n",
        "    self.weight_matrix = np.random.normal(scale=0.1,size=(v,d))\n",
        "  \n",
        "  def embed(self,words):\n",
        "    return np.dot(words,self.weight_matrix) # (n,V) x (V,d) = (n,d)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEHa-l0qAl_V"
      },
      "outputs": [],
      "source": [
        "def softmax(X):\n",
        "  exp_X = np.exp(X)\n",
        "  return exp_X/np.sum(exp_X,axis=1,keepdims=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjRlTahXgjHU"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  def attention(self,q,k,v):\n",
        "    dk = len(q[0])\n",
        "    return np.matmul(softmax(np.matmul(q,np.swapaxes(k, -1, -2))/np.sqrt(dk)),v) # (1,dk) x (dk,1) x (1,dv) = (1,1) x (1,dv) = (1,dv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKRpxvERczvm"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention():\n",
        "  def __init__(self,dmodel,h):\n",
        "    self.h=h\n",
        "    self.dk = dmodel//h\n",
        "    self.dv = dmodel//h\n",
        "    self.WQ = []\n",
        "    self.WK = []\n",
        "    self.WV = []\n",
        "    self.WO = np.random.normal(scale=0.1, size=(h*self.dv, dmodel))\n",
        "    self.sdpa = ScaledDotProductAttention()\n",
        "    for i in range(h):\n",
        "      self.WQ.append(np.random.normal(scale=0.1, size=(dmodel, self.dk)))\n",
        "      self.WK.append(np.random.normal(scale=0.1, size=(dmodel, self.dk)))\n",
        "      self.WV.append(np.random.normal(scale=0.1, size=(dmodel, self.dv)))\n",
        "    self.WQ = np.array(self.WQ)\n",
        "    self.WK = np.array(self.WK)\n",
        "    self.WV = np.array(self.WV)\n",
        "    print(\"WQ shape : \",self.WQ.shape)\n",
        "    pass\n",
        "\n",
        "  def MultiHead(self,Q,K,V):\n",
        "    head = []\n",
        "    for i in range(self.h):\n",
        "      q = np.dot(Q,self.WQ[i])\n",
        "      k = np.dot(K,self.WK[i])\n",
        "      v = np.dot(V,self.WV[i])\n",
        "      head.append(self.sdpa.attention(q,k,v)) \n",
        "    head = np.array(head)\n",
        "    head = np.concatenate(head,axis=1)\n",
        "    multihead = np.dot(head,self.WO) # (dmodel,dv*h)\n",
        "    print(\"Multi-Head Shape : \",multihead.shape)\n",
        "    return multihead\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ua_bX8Pvc6HW"
      },
      "outputs": [],
      "source": [
        "class AddAndNorm():\n",
        "  def __init__(self,feat_size):\n",
        "    self.epsilon = 1e-8\n",
        "    self.gamma = np.ones(feat_size)\n",
        "    self.beta = np.zeros(feat_size)\n",
        "    pass\n",
        "  def add(self,x,y):\n",
        "    return x+y\n",
        "  def norm(self,x):\n",
        "    #print(\"Norming\")\n",
        "    return np.multiply(self.gamma,(x-np.mean(x))) / np.sqrt(np.var(x)+self.epsilon)+self.beta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLbWuRUxc_g2"
      },
      "outputs": [],
      "source": [
        "class FeedForward():\n",
        "  def __init__(self,d_ff,d_model):\n",
        "    self.d_ff=d_ff\n",
        "    self.W1 = np.random.normal(scale=np.sqrt(2.0 / d_model), size=(d_model, d_ff))\n",
        "    self.W2 = np.random.normal(scale=np.sqrt(2.0 / d_ff), size=(d_ff, d_model))\n",
        "    self.b1 = np.zeros((d_ff,))\n",
        "    self.b2 = np.zeros((d_model,))\n",
        "  \n",
        "  def forward(self,x):\n",
        "    return np.dot(np.maximum(np.dot(x,self.W1)+self.b1,0),self.W2) +self.b2\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sG1zaSb0Ymfk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGev7DpidFkV"
      },
      "outputs": [],
      "source": [
        "class Encoder():\n",
        "  def __init__(self,vocab_size,dmodel=512,h=8,d_ff=2048):\n",
        "    self.ie = InputEmbedding(vocab_size,dmodel)\n",
        "    self.mha = MultiHeadAttention(dmodel,h)\n",
        "    self.ffn = FeedForward(d_ff,dmodel)\n",
        "    self.an = AddAndNorm((1,dmodel))\n",
        "    \n",
        "  def encode(self,X):\n",
        "    embedded_sentence = self.ie.embed(X)\n",
        "    print(\"Embedded Sentence\")\n",
        "    n = len(embedded_sentence)\n",
        "    multi_head_op = self.mha.MultiHead(embedded_sentence,embedded_sentence,embedded_sentence)\n",
        "    print(\"Performed Multi-Headed Attention\")\n",
        "    print(\"Multi Head OP shape : \",multi_head_op.shape)\n",
        "    mhan=self.an.norm(self.an.add(embedded_sentence,multi_head_op))\n",
        "    ffn_op = self.ffn.forward(mhan)\n",
        "    print(\"Passed through Feed-Forward Layer\")\n",
        "    op = self.an.norm(self.an.add(mhan,ffn_op))\n",
        "    return op \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0jwZNIpyoIj",
        "outputId": "d562aab9-a066-4ad9-b60f-8489bfc78f60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<U6\n"
          ]
        }
      ],
      "source": [
        "print(np.array(encoded.tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS57pVydBMj1",
        "outputId": "44b065ce-0228-4a17-c5ca-4bc07d1bebd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30000\n"
          ]
        }
      ],
      "source": [
        "vocab_size = tokenizer.get_vocab_size()\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X12vpl9BHr_",
        "outputId": "38d24ed0-2ece-4923-9bf9-763c80feb4e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WQ shape :  (8, 512, 64)\n"
          ]
        }
      ],
      "source": [
        "encoder = Encoder(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahNgljJVCtx0",
        "outputId": "cc706398-7962-43bb-9a2f-3d02ea842722"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoded sentence: This is a test sentence.\n",
            "Word ID: 1344\n"
          ]
        }
      ],
      "source": [
        "sentence = \"This is a test sentence.\"\n",
        "encoded_ids = tokenizer.encode(sentence).ids\n",
        "\n",
        "# Convert the BPE encoded tokens back into a sentence\n",
        "decoded_sentence = tokenizer.decode(encoded_ids)\n",
        "print(\"Decoded sentence:\", decoded_sentence)\n",
        "\n",
        "# Get the word ID of a specific word in the sentence\n",
        "word = \"test\"\n",
        "word_id = tokenizer.get_vocab().get(word)\n",
        "print(\"Word ID:\", word_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmZlvgT-CrtM",
        "outputId": "c9cd0ef2-6b1a-42a6-8092-0e0cd3fc860b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6, 30000)\n"
          ]
        }
      ],
      "source": [
        "sent = \"This is an apple\"\n",
        "word_ids = tokenizer.encode(sentence).ids\n",
        "one_hot_matrix = np.zeros((len(word_ids),vocab_size))\n",
        "for i,word_id in enumerate(word_ids):\n",
        "  one_hot_vector = np.zeros((vocab_size,))\n",
        "  one_hot_vector[word_id] = 1\n",
        "  one_hot_matrix[i] = one_hot_vector\n",
        "print(one_hot_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTiRosTLEWuU"
      },
      "outputs": [],
      "source": [
        "input = one_hot_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oibncyAiBawE",
        "outputId": "a3248a7b-787b-443f-f784-0ea2e0ab6dee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedded Sentence\n",
            "Multi-Head Shape :  (6, 512)\n",
            "Performed Multi-Headed Attention\n",
            "Multi Head OP shape :  (6, 512)\n",
            "Passed through Feed-Forward Layer\n",
            "[[-0.54822146 -1.71099858  0.25149148 ... -2.72457476  0.01477995\n",
            "  -0.97032392]\n",
            " [ 0.13263687 -0.82482058  0.06157339 ... -2.82808578  1.0061383\n",
            "  -1.2784061 ]\n",
            " [-0.49605064 -1.23053718  0.5871976  ... -2.2064601   0.51385374\n",
            "  -0.35642698]\n",
            " [ 0.00633263 -0.81093749  0.55840952 ... -1.53274095  0.14538025\n",
            "  -0.13842284]\n",
            " [-0.09512293 -0.80831779  0.74561569 ... -1.9349708   0.0088119\n",
            "  -0.75519074]\n",
            " [-0.10438281 -0.93523135  0.35014946 ... -1.58002955  0.900787\n",
            "  -0.50915748]]\n"
          ]
        }
      ],
      "source": [
        "encoder_op = encoder.encode(input)\n",
        "print(encoder_op)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTmN_1FoBgcs",
        "outputId": "eed3c8b2-1c97-4639-e905-3e589b888dd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6, 512)\n"
          ]
        }
      ],
      "source": [
        "print(encoder_op.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5HpHJ5q0fQZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
