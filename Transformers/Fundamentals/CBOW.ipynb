{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch87YAjXQS3i",
        "outputId": "9da8ab23-2e1e-4d88-ba14-ab4549ff23af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to\n",
            "[nltk_data]     C:\\Users\\shahj\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk \n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown \n",
        "from scipy.sparse import csr_matrix\n",
        "import scipy.sparse as sp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4R6geApQS3k",
        "outputId": "6a5be1d3-3dc4-42b1-cdc1-d2c4e32aaa98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]\n"
          ]
        }
      ],
      "source": [
        "corpus = brown.sents()\n",
        "print(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "YYB1d3P3QS3k"
      },
      "outputs": [],
      "source": [
        "processed_corpus = [[word.lower() for word in sent] for sent in corpus]\n",
        "#print(processed_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "wgFM1qGsQS3k"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "word_dic = {}\n",
        "word_vec_dim = 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "Dn5SpxNjQS3l"
      },
      "outputs": [],
      "source": [
        "# def getRandomGaussian(u,sd,x):\n",
        "#     return (math.e**((((x-u)/sd)**2)/2))/(sd*math.sqrt(2*math.pi))  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "HkNYjcPkQS3l"
      },
      "outputs": [],
      "source": [
        "word_id =0\n",
        "for sent in processed_corpus:\n",
        "    for word in sent:\n",
        "        if word in word_dic: continue\n",
        "        word_dic[word] = word_id\n",
        "        word_id +=1\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "aa1-4FZfQS3l"
      },
      "outputs": [],
      "source": [
        "vocabulary_size = len(word_dic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "4dhPW7XLQS3l"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "u = 0\n",
        "sd = 1/math.sqrt(word_vec_dim)\n",
        "word_embeddings = u + sd*np.random.randn(len(word_dic),word_vec_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7h80QudbQS3m",
        "outputId": "156aca6f-ee84-455b-fc3e-bbe0824c8eb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-0.1289508  -0.06136935  0.04578894 ...  0.0747346   0.12536673\n",
            "   0.13008117]\n",
            " [-0.09354144  0.01310806  0.0257003  ... -0.02891173 -0.24521245\n",
            "   0.03299929]\n",
            " [-0.03685201  0.10138862  0.12027476 ... -0.12634874 -0.06948295\n",
            "   0.2261163 ]\n",
            " ...\n",
            " [-0.10494674 -0.18970313 -0.06689823 ... -0.01785863  0.02529991\n",
            "  -0.11611051]\n",
            " [ 0.00148489 -0.00292931 -0.01029188 ...  0.18702946 -0.03548352\n",
            "   0.03482034]\n",
            " [ 0.0539474  -0.12849745  0.12302965 ... -0.06791575  0.03128248\n",
            "   0.03810832]]\n"
          ]
        }
      ],
      "source": [
        "print(word_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "Z8hc2PNYEZS7"
      },
      "outputs": [],
      "source": [
        "w2 = u + sd*np.random.randn(word_vec_dim,len(word_dic))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aksn1BgCFlhk",
        "outputId": "cbbfafa3-786c-4ca9-da66-e37af77cbb10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(100, 49815)\n"
          ]
        }
      ],
      "source": [
        "print(w2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "wW3XY6Dq3o2M"
      },
      "outputs": [],
      "source": [
        "#Thanks ChatGPT\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "def gen_data_set(corpus, window, word_dic):\n",
        "    X = []\n",
        "    y = []\n",
        "    vocabulary_size = len(word_dic)\n",
        "    row = []\n",
        "    col = []\n",
        "    data = []\n",
        "    row_y = []\n",
        "    col_y = []\n",
        "    data_y = []\n",
        "    training_sample = 0\n",
        "    for sent in corpus:\n",
        "        for tw_index in range(len(sent)):\n",
        "            target_word = sent[tw_index]\n",
        "            context = sent[max(0, tw_index - window):tw_index] + sent[tw_index + 1:min(len(sent), tw_index + window + 1)]\n",
        "            for cw_index in range(len(context)):\n",
        "                context_word = context[cw_index]\n",
        "                row.append(training_sample)\n",
        "                col.append(word_dic[context_word])\n",
        "                data.append(1)\n",
        "            row_y.append(training_sample)\n",
        "            col_y.append(word_dic[target_word])\n",
        "            data_y.append(1)\n",
        "            training_sample+=1\n",
        "\n",
        "    X = csr_matrix((data, (row, col)), shape=(training_sample, vocabulary_size))\n",
        "    #y = [word_dic[word] for sent in corpus for word in sent]\n",
        "    y = csr_matrix((data_y, (row_y, col_y)), shape=(training_sample, vocabulary_size))\n",
        "    print(y)\n",
        "    return X, y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "X7x9jsejQS3m"
      },
      "outputs": [],
      "source": [
        "# window = 5\n",
        "# def gen_data_set(corpus,window,word_dic):\n",
        "#   X = []\n",
        "#   y = []\n",
        "#   vocabulary_size = len(word_dic)\n",
        "#   for sent in corpus:\n",
        "#       for tw_index in range(len(sent)):\n",
        "#           context_vec = [0 for i in range(vocabulary_size)]\n",
        "#           target_word = sent[tw_index]\n",
        "#           context = sent[max(0,tw_index-window):tw_index] + sent[tw_index+1:min(len(sent),tw_index+window+1)]\n",
        "#           for cw_index in range(max(0,tw_index-window),tw_index):\n",
        "#               context_word = sent[cw_index]\n",
        "#               context_vec[word_dic[context_word]] += 1\n",
        "#           X.append(context_vec)\n",
        "#           y.append(word_dic[target_word])\n",
        "#           #print(context)\n",
        "#           #break\n",
        "#   return X,y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "AdIurBkk0m-I"
      },
      "outputs": [],
      "source": [
        "window = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "thVXn0OMQS3m"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 0)\t1\n",
            "  (1, 1)\t1\n",
            "  (2, 2)\t1\n",
            "  (3, 3)\t1\n",
            "  (4, 4)\t1\n",
            "  (5, 5)\t1\n",
            "  (6, 6)\t1\n",
            "  (7, 7)\t1\n",
            "  (8, 8)\t1\n",
            "  (9, 9)\t1\n",
            "  (10, 10)\t1\n",
            "  (11, 11)\t1\n",
            "  (12, 12)\t1\n",
            "  (13, 13)\t1\n",
            "  (14, 14)\t1\n",
            "  (15, 15)\t1\n",
            "  (16, 16)\t1\n",
            "  (17, 17)\t1\n",
            "  (18, 18)\t1\n",
            "  (19, 19)\t1\n",
            "  (20, 20)\t1\n",
            "  (21, 21)\t1\n",
            "  (22, 22)\t1\n",
            "  (23, 23)\t1\n",
            "  (24, 24)\t1\n",
            "  :\t:\n",
            "  (1161167, 24)\t1\n",
            "  (1161168, 254)\t1\n",
            "  (1161169, 550)\t1\n",
            "  (1161170, 723)\t1\n",
            "  (1161171, 44)\t1\n",
            "  (1161172, 1389)\t1\n",
            "  (1161173, 56)\t1\n",
            "  (1161174, 23432)\t1\n",
            "  (1161175, 26)\t1\n",
            "  (1161176, 67)\t1\n",
            "  (1161177, 12183)\t1\n",
            "  (1161178, 32)\t1\n",
            "  (1161179, 15437)\t1\n",
            "  (1161180, 15281)\t1\n",
            "  (1161181, 32)\t1\n",
            "  (1161182, 0)\t1\n",
            "  (1161183, 3952)\t1\n",
            "  (1161184, 2009)\t1\n",
            "  (1161185, 0)\t1\n",
            "  (1161186, 49812)\t1\n",
            "  (1161187, 49813)\t1\n",
            "  (1161188, 7178)\t1\n",
            "  (1161189, 44)\t1\n",
            "  (1161190, 49814)\t1\n",
            "  (1161191, 24)\t1\n"
          ]
        }
      ],
      "source": [
        "X,y = gen_data_set(processed_corpus,window,word_dic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmKNtq5S6iNB",
        "outputId": "409f2a2c-8b6e-49b3-db4b-57cdcc9e2496"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(49815, 100)\n"
          ]
        }
      ],
      "source": [
        "print(word_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5fI6QTD5E_e",
        "outputId": "8d30c394-ee0a-41c2-bd92-4fe4a53938e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1161192, 49815)\n"
          ]
        }
      ],
      "source": [
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtpheUfU5Hwt",
        "outputId": "58712bb2-a548-485c-bf54-b0b74a0f5f31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "49815\n"
          ]
        }
      ],
      "source": [
        "print(vocabulary_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1161192, 49815)\n"
          ]
        }
      ],
      "source": [
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "xjP0uO5f51DH"
      },
      "outputs": [],
      "source": [
        "# y = np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "Am6j-4h56EvY"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    def __init__(self,weights,activation):\n",
        "      self.weights = weights\n",
        "      self.activation = activation\n",
        "      self.out = []\n",
        "\n",
        "class NN:\n",
        "  def __init__(self,layers):\n",
        "    self.layers = []\n",
        "    for l in layers:\n",
        "      self.layers.append(l)\n",
        "  \n",
        "  def activation(self,name):\n",
        "    def identity(x):\n",
        "      return x\n",
        "    def SoftMax(X):\n",
        "      X=X.todense()\n",
        "      print(\"X shape is \",X.shape)\n",
        "      print(X)\n",
        "      denom = sum(np.exp(X))\n",
        "      print(\"Denominator is \",denom)\n",
        "      out = np.exp(X)/denom\n",
        "      print(sum(out))\n",
        "      return out\n",
        "\n",
        "    if name == \"SoftMax\":\n",
        "      return SoftMax\n",
        "    else:\n",
        "      return identity\n",
        "    \n",
        "      \n",
        "  def ac_grad(self,name):\n",
        "    def identity(x):\n",
        "      return 1\n",
        "    def grad_SoftMax(X):\n",
        "      sm = self.activation('SoftMax')(X)\n",
        "      return np.dot(sm,1-sm.T) # Still unclear, Revisit this and re verify\n",
        "\n",
        "    if name == 'SoftMax':\n",
        "      return grad_SoftMax\n",
        "    else:\n",
        "      return identity\n",
        "\n",
        "  def loss(self,name):\n",
        "    print(\"Calculating loss.\",end=\"\")\n",
        "    def bce(y,prob_y):\n",
        "      #print(\" Verifying shapes : \",y.shape,prob_y.shape)\n",
        "      epsilon = 1e-10\n",
        "      prob_y = np.clip(prob_y, epsilon, 1 - epsilon)\n",
        "      print(\".\",end=\"\")\n",
        "      # N = len(y)\n",
        "      N = y.shape[1]\n",
        "      y = y.todense()\n",
        "      prob_y = prob_y\n",
        "      print(\".\",end=\"\")\n",
        "      # summation = 0\n",
        "      # for i in range(len(y)):\n",
        "      #   summation+= (y[i]*np.log(prob_y[i]) + (1-y[i])*np.log(1-prob_y[i]))\n",
        "      H = (-1/N)*np.sum(np.multiply(y,np.log(prob_y)) + np.multiply((1-y),np.log(1-prob_y)))\n",
        "      print(\".\")\n",
        "      return H\n",
        "    if name == 'BinaryCrossEntropy':\n",
        "      return bce\n",
        "  \n",
        "  def grad(self,loss):\n",
        "    print(\"Getting the gradient of loss .\",end=\"\")\n",
        "    def bce(y,prob_y):\n",
        "      y = y.todense()\n",
        "      print(\".\",end=\"\")\n",
        "      epsilon = 1e-10\n",
        "      prob_y = np.clip(prob_y, epsilon, 1 - epsilon)\n",
        "      print(\".\",end=\"\")\n",
        "      op=y/prob_y - (1-y)/(1-prob_y)\n",
        "      print(\".\")\n",
        "      return op\n",
        "    if loss == 'BinaryCrossEntropy':\n",
        "      return bce\n",
        "\n",
        "  def forward(self,X):\n",
        "    #from scipy.sparse import csr_matrix\n",
        "    print(\"Forward step\")\n",
        "    print(\"Weights shape \",self.layers[0].weights.shape)\n",
        "    print(\"Input Shape \",X.shape)\n",
        "    # Convert the weight matrix to a sparse CSR matrix\n",
        "    weights_sparse = csr_matrix(self.layers[0].weights)\n",
        "\n",
        "    # Perform the sparse matrix multiplication\n",
        "    X=X.reshape(1,-1)\n",
        "    #print(X)\n",
        "    print(\"Input Shape \",X.shape)\n",
        "    prod= np.dot(X, weights_sparse)\n",
        "    #print(\"Product is :\",prod)\n",
        "    out0 = prod\n",
        "    #out0 = self.activation(self.layers[0].activation)(np.dot(X,self.layers[0].weights))\n",
        "    print(\"Out0 done\")\n",
        "    self.layers[0].out = out0\n",
        "    print(out0.shape)\n",
        "    print(\"Weights2 shape \",self.layers[1].weights.shape)\n",
        "    weights2_sparse = csr_matrix(self.layers[1].weights)\n",
        "    prod2 = np.dot(out0,weights2_sparse)\n",
        "    print(\"Prod2 shape :\",prod2.shape)\n",
        "    out = self.activation(self.layers[1].activation)(prod2)\n",
        "    print(\"Out1 done\")\n",
        "    self.layers[1].out = out\n",
        "    return out\n",
        "  \n",
        "  def backward(self,X,y):\n",
        "    print(\"Backward step\")\n",
        "    L = self.loss('BinaryCrossEntropy')(y,self.layers[1].out)\n",
        "    dLdy = self.grad('BinaryCrossEntropy')(y,self.layers[1].out)\n",
        "    weights2_sparse = csr_matrix(self.layers[1].weights)\n",
        "    dydx2 = self.ac_grad(self.layers[1].activation)(np.dot(self.layers[0].out,weights2_sparse))\n",
        "    dLdw1 = np.dot(dLdy,np.dot(dydx2,self.layers[0].out))\n",
        "    dLdy0 = np.dot(dLdy,np.dot(dydx2,weights2_sparse))\n",
        "    weights_sparse = csr_matrix(self.layers[0].weights)\n",
        "    dy0dx1 = np.dot(X,weights_sparse)\n",
        "    print(X.shape, dLdy0.shape, dy0dx1.shape)\n",
        "    dLdw0 = np.dot(X,np.dot(dLdy0,dy0dx1))\n",
        "    \n",
        "    return dLdw0, dLdw1, L\n",
        "  \n",
        "  def backprop(self,X,y,alpha=0.1,max_iter=1000):\n",
        "    print(\"Beginning Backprop\")\n",
        "    losses = []\n",
        "    for iter in range(max_iter):\n",
        "      for i in range(X.shape[0]):\n",
        "        row = X[i,:]\n",
        "        target = y[i]\n",
        "        self.forward(row)\n",
        "        dLdw0, dLdw1, L = self.backward(row,target)\n",
        "        self.layers[0].weights -= alpha*dLdw0\n",
        "        self.layers[1].weights -= alpha*dLdw1\n",
        "        losses.append(L)\n",
        "        del row\n",
        "        del target\n",
        "        break\n",
        "      if abs(L - losses[-1])< 1e-5: break \n",
        "    return losses\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "rY4PFCE0VzAZ"
      },
      "outputs": [],
      "source": [
        "nn = NN(\n",
        "    [\n",
        "        Layer(word_embeddings,'None'),\n",
        "        Layer(w2,'SoftMax')\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7-JGiO7WPRA",
        "outputId": "df336d5a-838d-46d3-ef0d-af9d358a4e32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning Backprop\n",
            "Forward step\n",
            "Weights shape  (49815, 100)\n",
            "Input Shape  (1, 49815)\n",
            "Input Shape  (1, 49815)\n",
            "Out0 done\n",
            "(1, 100)\n",
            "Weights2 shape  (100, 49815)\n",
            "Prod2 shape : (1, 49815)\n",
            "X shape is  (1, 49815)\n",
            "[[ 0.15564072  0.01536633  0.15090128 ... -0.24190248  0.04127013\n",
            "  -0.09676485]]\n",
            "Denominator is  [[1.16840634 1.015485   1.16288185 ... 0.78513274 1.04213358 0.90776944]]\n",
            "[[1. 1. 1. ... 1. 1. 1.]]\n",
            "Out1 done\n",
            "Backward step\n",
            "Calculating loss....\n",
            "Getting the gradient of loss ....\n",
            "X shape is  (1, 49815)\n",
            "[[ 0.15564072  0.01536633  0.15090128 ... -0.24190248  0.04127013\n",
            "  -0.09676485]]\n",
            "Denominator is  [[1.16840634 1.015485   1.16288185 ... 0.78513274 1.04213358 0.90776944]]\n",
            "[[1. 1. 1. ... 1. 1. 1.]]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "shapes (1,49815) and (1,1) not aligned: 49815 (dim 1) != 1 (dim 0)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[131], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m losses \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mbackprop(X,y,\u001b[39m0.1\u001b[39;49m,\u001b[39m1\u001b[39;49m)\n",
            "Cell \u001b[1;32mIn[129], line 130\u001b[0m, in \u001b[0;36mNN.backprop\u001b[1;34m(self, X, y, alpha, max_iter)\u001b[0m\n\u001b[0;32m    128\u001b[0m target \u001b[39m=\u001b[39m y[i]\n\u001b[0;32m    129\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(row)\n\u001b[1;32m--> 130\u001b[0m dLdw0, dLdw1, L \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackward(row,target)\n\u001b[0;32m    131\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mweights \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m alpha\u001b[39m*\u001b[39mdLdw0\n\u001b[0;32m    132\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mweights \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m alpha\u001b[39m*\u001b[39mdLdw1\n",
            "Cell \u001b[1;32mIn[129], line 113\u001b[0m, in \u001b[0;36mNN.backward\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    111\u001b[0m weights2_sparse \u001b[39m=\u001b[39m csr_matrix(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mweights)\n\u001b[0;32m    112\u001b[0m dydx2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mac_grad(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mactivation)(np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mout,weights2_sparse))\n\u001b[1;32m--> 113\u001b[0m dLdw1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(dLdy,np\u001b[39m.\u001b[39;49mdot(dydx2,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mout))\n\u001b[0;32m    114\u001b[0m dLdy0 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(dLdy,np\u001b[39m.\u001b[39mdot(dydx2,weights2_sparse))\n\u001b[0;32m    115\u001b[0m weights_sparse \u001b[39m=\u001b[39m csr_matrix(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mweights)\n",
            "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: shapes (1,49815) and (1,1) not aligned: 49815 (dim 1) != 1 (dim 0)"
          ]
        }
      ],
      "source": [
        "losses = nn.backprop(X,y,0.1,1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "fbfbffb4b8bfba404249342476d0d4098caa2ba1a12dea0e50e12817ed7115eb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
